Title:  Residual Networks
Date:   2016-05-11
Category: neural networks

# Abstract 

Understanding the content of an image is a long-standing challenge in computer vision. 
In the last five years, the state-of-the art has advanced far beyond scanning checks and facial recognition to the point where, on certain benchmarks, object classification performed by a machine exceeds human level accuracy.

A representative benchmark is the [ImageNet Large Scale Visual Recognition Challenge](www.image-net.org/challenges/LSVRC/) which provides 1.2 million training images for identifying 1000 classes of objects. Companies like Facebook, Google, and Microsoft use this challenge as a proving ground for technology that will monetize images. In 2014 Google won the ImageNet competition with their "Inception" network and an accuracy of 6.7%. The winning network in 2015 was Microsoft Research's "ResNet" with an accuracy of 3.6%.

The winning architecture is notable because it commands attention simply by it's striking improvement over last year's result. 
Second, the improvement was achieved using a reformulated network structure rather than by using additional parameters to learn coupled with additional computation. 
Third, their ResNet is simpler than [Inception](https://arxiv.org/abs/1409.4842), which won in 2014, and has opened a new path for research and future improvements. 

# Technical Summary of He's Presentation

The Microsoft Research team of Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun won the 2015 ImageNet competition in a number of categories. 
In the object detection task an ensemble of residual models achieved a 3.6% classification error rate.

## Deeper Models

Anticipating their 152 layer winning network (the deepest to date), He et al. noticed each winning network was deeper than the year before. This raises the question they pose, "Is learning better networks as simple as stacking more layers?"

![ImageNet Presentation Slide 6](/images/ilsvrc2015_he-006.jpg)
![ImageNet Presentation Slide 3](/images/ilsvrc2015_he-003.jpg)


The answer is "yes" and "no". 
He showed  that adding traditional layers made a model perform worse (no). 
However, with a tweaked architecture adding more layers improves performance
and the “deeper is better” paradigm is preserved (yes).

![ImageNet Presentation Slide 22](/images/ilsvrc2015_he-022.jpg)

A basic residual block consists of two convolution layers with a shortcut connection 
across both layers.
Let the two weight layers constitute the function F(x) and 
the desired function be H(x). 
For a plain network, the weights learn H(x). 
In a residual network the weights learn H(x)-x. 

![ImageNet Presentation Slide 17](/images/ilsvrc2015_he-017.jpg)

# Not too deep enough

If deeper models are better, why stop at 152 layers?
The authors tried an "agressively deep" model with 1202 layers 
but it's error was higher. 

# Code

If you want to see some code... T 

### Original Recipe, Caffe

[Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks )

### Facebook, Torch 

[ResNet training in Torch](https://github.com/facebook/fb.resnet.torch)

### Yours Truly, Keras

Works with Theano or TensorFlow as the backend. 

[Keras ResNet](https://github.com/roryhr/keras_resnet)

